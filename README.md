# Image captioning (Генерация текстового описания изображений)
<br>
В данном проекте представлена детальная методология предварительной обработки данных с целью их подготовки перед обучением моделей. Кроме того, реализованы два ведущих архитектурных решения в области image captioning, которые находятся в центре внимания исследовательского сообщества. Помимо этого, мной был создан веб-сервис, обеспечивающий возможность загрузки изображений и генерации двух описаний для каждой из обученных архитектур (модель LSTM + ResNet и модель LSTM + MobileNet + Soft Attention).<br>

---
## Краткая теория:
 Архитектура для создания image captioning обычно основывается на сочетании сверточных нейронных сетей (Convolutional Neural Networks, CNN) для обработки изображения и рекуррентных нейронных сетей (Recurrent Neural Networks, RNN) для генерации подписи. То есть данная задача по созданию текстовых описаний является комбинацией сразу двух популярных направлений в Deep Learning (NLP + CV).

 Классический подход к image captioning с использованием RNN (например, LSTM - Long Short-Term Memory) имеет некоторые проблемы. Одна из основных проблем - это то, что RNN имеет фиксированную длину контекста и обрабатывает информацию последовательно. Это означает, что RNN видит только небольшой контекст, поэтому при генерации слов часто забывет старые результаты своей генерации. Так же такой подход не позволяет полностью учесть контекст изображения на каждом этапе генерации. В данном случае выделенные фичи изображения с помощью CNN лишь единожды подаются на вход LSTM блока, так что со временем генерация полностью теряет память о исходном изображении и начинает "додумывать самостоятельно".

 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1XUfYNlfE0j-sCWgir84GyjMzDsSzUbUR" alt="bot" width=660" height="435">
                                                                                                              
  __Рисунок 1 - Архитектура модели LSTM + ResNet__
</div>
                                                                                            
                                                                                                               
<br>
Для решения этих описанных ранее проблем применяется механизм внимания (attention mechanism). В случае image captioning, механизм внимания позволяет сети "обращаться" к различным частям изображения на каждом шаге генерации текста. Таким образом контекст самого изображения не теряется со временем генерации. А представленный механизм soft attention позволяет сети фокусироваться на разных частях изображения с разной степенью "важности" на каждом шаге генерации, что значительно увеличивает качество финального описания. Так же такие текста по большей части являются полее содержательными, а главное оконченными (В случае первой модели зачастую происходит зацикливание содердания из-за потери контекста).

 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1ARHvm2TAWAqY8sp0bV29BGgjG3bFqYmQ" alt="bot" width=790" height="430">
  
 __Рисунок 2 - Архитектура модели LSTM + MobileNet + Soft Attention__
</div>

---
## Структура проекта:
Подробное описание процесса предподготовки изображений и тестовых описаний для обучения сетей, обучение сети с архитектурой LSTM + ResNet34, а так же реализация инференции (inference) данной модели при подаче нового незнакомого изображения представлены в jupiter notebook - [__image_captioning_no_attention.ipynb__](https://nbviewer.org/github/Koldim2001/Image_captioning/blob/main/image_captioning_no_attention.ipynb)    <br>

Обучение сети с архитектурой LSTM + MobileNet + Soft Attention, а так же реализация инференции (inference) данной модели при подаче нового незнакомого изображения представлены в jupiter notebook - [__image_captioning_with_attention.ipynb__](https://nbviewer.org/github/Koldim2001/Image_captioning/blob/main/image_captioning_with_attention.ipynb)<br>

Вэб сервис я реализовал с помощью веб-фреймворка Streamlit, предназначенного для простого развертывания моделей. Скрипт для запуска на localhost вэб приложения - [__web.py__](https://github.com/Koldim2001/Image_captioning/blob/main/web.py)

Помимо главных описанных файлов репозитория так же имеются .py файлы, в которых реализованы функции по инференсу моделей и загрузке обученных весов с моего Google Drive. Так же имеется файл с расширением .pkl, в котором сохранен полученный словарь в процессе предобраюотки текстовых описаний (NLP).

---
## Как запускать программу:
Данные команды требуется запускать последовательно в терминале:
1. Склонируйте к себе этот репозиторий 
```
git clone https://github.com/Koldim2001/Image_captioning.git
```
2. Перейдите с помощью команды cd в созданную папку Factory_detection
```
cd Image_captioning
```
3. Загрузите все необходимые библиотеки:
```
pip install -r requirements.txt
```
4. Запустите написанный вэб сервис:
```
streamlit run web.py --server.port 80
```
5. Перейдите на данный сайт: <br>

 >_Local URL:_ __http://localhost:80__

_PS: Для корректной работы streamlit веб-фреймворка может потребоваться наличие python версии не ниже 3.9.12 (то есть новее) .<br><br>_

  <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1q_MagU2P5S1jLY5HaAy5MAUEPF2ys2X0" alt="bot" width=410" height="645">
    <img src="https://drive.google.com/uc?id=1CUXAacC1t8An8UyzDTMtsgfdjD6BuQ67" alt="bot" width=410" height="645">
 <div style="text-align:center;">
  <img src="https://drive.google.com/uc?id=1E77zIF1yq9m6F6Q-vtQRCjWp-6v3UdRX" alt="bot" width=410" height="645">
 <img src="https://drive.google.com/uc?id=1qk-LqR-LO00B34jIBwvWNB5Fz3QWgFVz" alt="bot" width=410" height="645">


  
  __Рисунок 3 - Примеры работы веб-приложения__
</div>
